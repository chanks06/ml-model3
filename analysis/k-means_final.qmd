---
title: "Kmeans_final"
output: html_document
date: "2023-04-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
library(tidymodels)
library(factoextra)
library(cluster)
library(plotly)
library(skimr)



# https://www.openml.org/d/1590

raw_income = read_csv("https://raw.githubusercontent.com/chanks06/ml-model3/main/datasets/openml_1590.csv", na=c("?"))

income = read_csv("https://raw.githubusercontent.com/chanks06/ml-model3/main/datasets/openml_1590.csv", na=c("?")) %>%
  drop_na() %>%
  mutate(income_above_50k = class==">50K") %>%
  select(-class) %>%
  dummy_cols(remove_selected_columns = T)

names(income)[3]<- "education_num"
names(income)[4]<- "capital_gain"
names(income)[5]<- "capital_loss"
names(income)[6]<- "hours_per_week"
names(income)[8]<- "workclass_fed_gov"
names(income)[9]<- "workclass_loc_gov"
names(income)[11]<- "workclass_self_emp_inc"
names(income)[12]<- "workclass_self_emp_not_inc"
names(income)[13]<- "workclass_state_gov"
names(income)[14]<- "workclass_without_pay"
```


### whole data and recipes
### Major Assumptions: flnwgt does not include age information

```{r}
# Drop target column and normalize data
income_features<- recipe(~ ., data = income) %>%
 step_rm(income_above_50k) %>%  ## will remove variables based on their name, type, or role
  step_naomit(everything(), skip = TRUE) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.
  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.


  prep() %>% 
  bake(new_data = NULL)

# Print out data
income_features %>% 
  slice_head(n = 5)
```

One way we can try to find out is to use a data sample to create a series of clustering models with an incrementing number of clusters, and measure how tightly the data points are grouped within each cluster. A metric often used to measure this tightness is the within cluster sum of squares (WCSS), with lower values meaning that the data points are closer. You can then plot the WCSS for each model.

We’ll use the built-in kmeans() function, which accepts a data frame with all numeric columns as it’s primary argument to perform clustering - means we’ll have to drop the species column. For clustering, it is recommended that the data have the same scale. We can use the recipes package to perform these transformations.

```{r}

set.seed(503)

# Create 10 models with 1 to 10 clusters
kclusts <- tibble(k = 1:10) %>% 
  mutate(
    model = map(k, ~ kmeans(x = income_features, centers = .x, nstart = 20)),
    glanced = map(model, glance)) %>% 
  unnest(cols = c(glanced))

# View results
kclusts
```



```{r}
# Plot Total within-cluster sum of squares (tot.withinss)
kclusts %>% 
  ggplot(mapping = aes(x = k, y = tot.withinss)) +
  geom_line(size = 1.2, alpha = 0.5, color = "dodgerblue3") +
  geom_point(size = 2, color = "dodgerblue3")+
  theme_minimal()
```


```{r}
set.seed(503)
# Fit and predict clusters with k = 3
final_kmeans <- kmeans(income_features, centers =3, nstart = 100, iter.max = 1000)

# Add cluster prediction to the data set
results <- augment(final_kmeans, income_features)

results %>% 
  slice_head(n = 5)
```



```{r}
results <- bind_cols(select(income, income_above_50k), as.data.frame(results))

```



```{r}
clust_spc_plot <- results %>% 
    ggplot(mapping = aes(x = age, y = fnlwgt)) +
    geom_point(aes(shape = .cluster, color= .cluster),size = 2,alpha=0.3)+ 

  scale_color_manual(values = c("darkorange","purple","cyan4"))+ theme_minimal()

# Make plot interactive
ggplotly(clust_spc_plot)

```

* discussion for choosing number of clusters
* analysis of cluster centers
* bivariate chart(s) against meaningful variables and/or analysis of density plots
* meaningful interpretation / discussion of conclusions
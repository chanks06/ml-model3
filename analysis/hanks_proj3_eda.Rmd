---
title: "Project 3"
author: "Charles Hanks & Karol Orozco"
date: "03/16/2023"
output:
  pdf_document:
    df_print: kable
    fig_width: 11
    fig_height: 8
  html_document:
    df_print: paged
---

## Setup
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_knit$set(root.dir = '/Users/charleshanks/repos/ml-model3/datasets')


library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
# https://www.openml.org/d/1590

options(scipen=999)
```

```{r}
raw_income = read_csv("openml_1590.csv", na=c("?"))

income = read_csv("openml_1590.csv", na=c("?")) %>%
  drop_na() %>%
  mutate(income_above_50k = class==">50K") %>%
  select(-class) %>%
  dummy_cols(remove_selected_columns = T)

#formatting col names: 
raw_income = raw_income %>% rename_all(funs(str_replace_all(.,"-","_"))) %>% 
                rename_all(funs(tolower(.)))

income = income %>% rename_all(funs(str_replace_all(.,"-","_"))) %>% 
                rename_all(funs(tolower(.)))
```


## Some questions about the data

Please try at least a few of the following:

* Run PCA on the dataset. How many principal components do you need to explain half the variation? 
* Can you give some interpretation of the principal components?
* Look at the scree plot. How many PCs would you choose based on that?
* Are the first few Principal Components good predictors of income_above_50k?
* How well can you predict income_above_50k using the first 5 or 6 principal components? How about only the first 2?
* Can you gain any insights into the data based on k-means clustering? 
* Can you visualize and interpret some or all of your clusters?
* Using any and all techniques we have learned, build the best predictive model for income_above_50k that you can. What are your best features? What model did you use? What interpretations can you draw?
* What metric can you use to assess model performance? Why is that a good choice of metric in this case?
* What are some key insights you found through your analysis?

Please remember: a statement like "PC2 is not a meaningful predictor for our modeling problem" is a great insight; sometimes things don't work!

10 pts PCA

* analysis and interpretation of factor loadings
* discussion of scree plot and/or analysis of some density plots of PCs
* meaningful interpretation / discussion of conclusions 

10 pts k-means

* discussion for choosing number of clusters
* analysis of cluster centers
* bivariate chart(s) against meaningful variables and/or analysis of density plots
* meaningful interpretation / discussion of conclusions 

10 pts supervised learning

* feature engineering / selection, whether with PCA or otherwise
* interpretation of variable importance, coefficients if applicable
* justification of choice of metric
* discussion of choice or tuning of hyperparameters, if any
* meaningful discussion of predictive power and conclusions from model

Please be prepared to 

* Submit your Rmd + compiled html or pdf, *and*
* Present your findings to the class in a compelling way, speaking for 10 minutes or so. You don't need to cover everything in your analysis that you submit to me, focus on the fun / interesting / compelling highlights or challenges.


####

# BACKGROUND

- Prediction task is to determine whether a person makes over 50k a year. Data from 1994 Census database. 
- fnlwgt: demographic background of the people - people with similar demographic characteristics should have similar weights. 


```{r}
library(skimr)
skim(raw_income)
```

# Log Transforming capital gain and capital loss: 

"You have a capital gain if you sell the asset for more than your adjusted basis. You have a capital loss if you sell the asset for less than your adjusted basis." <https://www.irs.gov/taxtopics/tc409#:~:text=You%20have%20a%20capital%20gain,%2C%20aren't%20tax%20deductible.>

```{r}
ggplot(income, aes(x = capital_loss)) + geom_histogram()
#definition: capital gain refers to the increase in the value of a capital asset when it is sold. A capital gain occurs when you sell an asset for more than what you originally paid for it. 

income = income %>% mutate(l_capital_gain = log(capital_gain), 
                  l_capital_loss = log(capital_loss)) %>% 
                        select(-capital_gain,-capital_loss)

#moving these cols back to front of dummy cols: 
income = income %>% relocate(l_capital_gain, l_capital_loss)

#changing -Inf back to 0 for log transformed vars: 
income = income %>% mutate(l_capital_gain = ifelse(l_capital_gain == -Inf, 0, l_capital_gain),
                  l_capital_loss = ifelse(l_capital_loss == -Inf, 0, l_capital_loss))

#Now checking out shape of log transformed data: 
ggplot(income, aes(x = l_capital_gain)) + geom_histogram() + xlim(0,12)
ggplot(income, aes(x = l_capital_loss)) + geom_histogram() + xlim(0,12)
```

# Exploratory Data Analysis 

```{r}
#what is education_num? 
#is NA retired? Unemployed? 

#looking at age vs. occupation: 
raw_income %>% ggplot(aes(x = age, fill = occupation)) + geom_histogram() + facet_wrap(~occupation)
#military is a younger person's occupation 
#exec-manegerial is very normally distributed 
```

Distribution of careers by gender 
```{r}
raw_income %>% 
  group_by(occupation,sex) %>% 
          count() %>% 
          ggplot(aes(x = occupation, y = n, fill = sex)) + geom_col() + coord_flip()

#craft-repair, farming/fishing, executive mostly male, admin mostly female 
#there are some ethical considerations to unpack here in building this model...

```
education and type of work: 

```{r}
raw_income %>% group_by(education, workclass) %>% count() %>% ggplot(aes(x = n, y = education, fill = workclass)) + geom_col()
```
Examining distribution by sex: 
```{r}
raw_income %>% group_by(relationship) %>% summarize(avg_age= mean(age))

n_sex = raw_income %>% 
  group_by(sex) %>% 
    count()

n_sex %>%
    ggplot(aes(x = sex, y = n, fill = sex)) + geom_col()
#there are twice as many men in this dataset....may need to downsample by gender later in the game. This may lead to implicit bias toward men making more than 50k in the model. 

raw_income %>% group_by(sex, class) %>% count() %>% mutate(prop = n/nrow(raw_income))
#there is 1 female husband, look like this dataset does not take into account the difference between gender and sex
#Only 3.6 % of this dataset includes women who make more than 50k. 

```
# Age 

```{r}
ggplot(income, aes(x = income_above_50k, y = age, fill = income_above_50k)) + geom_boxplot()

income %>% group_by(income_above_50k) %>% summarize(med_age = median(age), avg_age = mean(age))

ggplot(income, aes(x = age)) + geom_histogram(binwidth = 10)

income = income %>% mutate(age_bin = 
                    case_when( 
                      age < 20 ~ "teen", 
                      age >=20 & age <30 ~ "20-29",
                      age >=30 & age <40 ~ "30-39", 
                      age >=40 & age <50 ~ "40-50",
                      age >=50 & age <66 ~ "50-65",
                      age >=65  ~ "65+")
                                          ) %>% 
                                    relocate(age_bin) %>% 
                                          relocate(l_capital_gain, l_capital_loss)

income$age_bin = factor(income$age_bin, levels = c("teen", "20-29","30-39","40-50","50-65","65+"))

#distribution of people by age bin
ggplot(income, aes(x = age_bin, fill = age_bin)) + geom_histogram(stat= 'count')
#the largest age group in this dataset is 30-39

ggplot(income, aes(x = age_bin)) + geom_histogram(stat = 'count',aes(fill = income_above_50k)) + facet_wrap(~income_above_50k)
#People in their forties are most likely to be making above 50k. 

```
Is there a correlation between education level and income over 50k ? 
```{r}
raw_income %>% ggplot(aes(x = class, y = fnlwgt)) + geom_col()

raw_income = raw_income %>% mutate(above_50k = ifelse(class == '<=50K',0,1))

cor.test(raw_income$`education-num`,raw_income$above_50k)

# correlation coeffcient is .33, not super strong.
```
What countries are represented in this dataset? 

```{r}
unique(raw_income$native_country)

#how many of each country? 

raw_income %>% group_by(native_country) %>% count() %>% arrange(desc(n)) %>% mutate(prop = n/nrow(raw_income))
#almost 90% are native citizen of USA, about 10 % have immigrate in their life time. 
#1.7 % do not have a native country listed. 

#I'm guessing that if you're white, male and from USA, there is a higher probability of making > 50k....


```

Is there a correlation between # of hours worked per week and income over 50k ? 

```{r}
cor.test(raw_income$hours_per_week, raw_income$above_50k)
#.22 correlation coef 

income = income %>% mutate(work_over40 = ifelse(hours_per_week > 40,1,0)) %>% relocate(work_over40) %>% relocate(l_capital_gain:hours_per_week)

#graph that 

income %>% group_by(work_over40, income_above_50k) %>% count() %>% ggplot(aes(x = work_over40, y = n, fill = income_above_50k)) + geom_col()

#answer: there are almost the same amount of folks who do / don't work over 40 hours who have income over 50k. However, the proportion of people who make over 50k and work over 40 is 40% that subgroup, compared to 18% of people who do make over 50 k and do not work over 40 hours per week. 

```


```{r}
age_bin_dummies = income %>% select(age_bin) %>% 
    dummy_cols(remove_selected_columns = T)

income = bind_cols(income,age_bin_dummies) %>% select(-age_bin)
```

# Principal Component Analysis on income dataset

We will apply Principal Component Analysis to our dataset in order to reduce the number of features needed to explain the variation in the data. We reduce the number of dimensions in our feature space through a linear combination of features that share co-variance. This process is based on a calculation of distances within the feature space. Is is therefore essential that we scale and center our numerical data. 
Instead of having to manually assess correlation among variables, we call the prcomp() function on our dataset to group our variables together.  

The result is that this new combination feature - the 'principal component' - captures the variation in the data according to the variables that it represents. Each principal component does _not_ correlate with another - they are as distinct from one another as possible. If we were to graph these linear combination of features, they would form a right angle extend out from the center of the data's spread.  

In practice, this allows us to use only a handful of principal components to explain how the data behaves. 

## prcomp() on income:

We need 38 principal components to explain half of the variation of the data.

```{r}
pr_income = prcomp(x = select(income,-income_above_50k), scale = T, center = T)

summary(pr_income)
pr_income$rotation
```

## Visualizing Principle Components 

Looking at at the plot below, we can see a visual ranking of how much of the variation each component captures. Each component is ranked according to its Eigenvalue. The eigenvalue is a measure of the proportion of variance explain by that component. 

The scree plot helps use select a cut off point for determining how we can explain the most with the fewest components as possible. We evaluate the slope of the line connecting the components, and find the point when the absolute value of the slope becomes small, and each component following continues that trend. The 'elbow' in the plot below is at PC5. The components after PC5 do much less for us in accounting for the variance the income dataset. 

```{r}
screeplot(pr_income, type = "lines")
#elbow is around pc5
```
## Interpretation of the Top 5 Principal Components 

We will look at the factor loadings for each component to determine what each component is telling us about the data. 

We can interpret factor loadings as coefficients of our linear combination for each feature within the principal component. They us the relative (transformed) value of each original feature. For example, a factor loading of -.75 for age would mean that this component contains the observations of younger people.  

```{r}
pr_income$rotation
```

## Component Interpretation 

We want to find the most influential original features within our top 5 PCs. To this we will filter to show only those where at least one of the PCs has a factor loading greater than or equal to the absolute value of 0.30. This gives us the highlights of each component. Here are descriptions of the type of workers these principal components: 

PC1: middle-aged husbands 
PC2: young men with little formal education
PC3: young, educated, and single (yo-pros) 
PC4: white Americans
PC5: non-white family members who don't work that much 

```{r}
rownames_to_column(as.data.frame(pr_income$rotation)) %>%
select(1:6) %>%
filter(abs(PC1) >= 0.25 | abs(PC2) >= 0.25 | abs(PC3) >= 0.25 | abs(PC4) >= 0.25 | abs(PC5) >= 0.25) %>% rename("husbands" = PC1, "low_ed_young_men" = PC2, "yopros" = PC3, "white_americans" = PC4, "non_white_fam_little_work" = PC5)

```

Let us now determine if these principle components are good predictors of income_above_50k. 

```{r}
prc = bind_cols(select(income, income_above_50k), as.data.frame(pr_income$x)) %>%
  select(1:6) %>%
    rename("husbands" = PC1, "low_ed_young_men" = PC2, "yopros" = PC3, "white_americans"= PC4, "non_white_fam_little_work" = PC5)

prc %>%
pivot_longer(cols = -income_above_50k, names_to = "component", values_to = "loading") %>% mutate(income_above_50k = as.factor(income_above_50k)) %>%
ggplot(aes(loading, fill=income_above_50k)) +
geom_density(alpha = 0.5) +
facet_grid(.~component)
```

Looking at the density plots above, it is clear that PC1 'husbands' is a good predictor for making over 50k income. The other components do not have enough delineation between the FALSE and TRUE mappings of income_above_50k. Among the other 4 components, there is too much overlap to consider them as good predictors of our reponse variable. We will incorporate PC1 into our classification model. 

How well can you predict income_above_50k using the first 5 or 6 principal components? How about only the first 2?

## Making a classification model using Components  

Let's make PC1 earn its dinner. What if we used only PC1 to predict class using a logistic regression model? 

Given the percentage of white men from America in this dataset, I think it will have a decently high accuracy, but overall low kappa. 

```{r}
prc$income_above_50k = factor(ifelse(prc$income_above_50k == 'FALSE', 'no','yes'),levels = c('no','yes'))

prc.pc1 = prc %>% select(income_above_50k, husbands)

ctrl <- trainControl(method = "cv", number = 3, classProbs = TRUE)

prc.pc1_index <- createDataPartition(prc.pc1$income_above_50k, p = 0.80, list = FALSE)
train <- prc.pc1[prc.pc1_index, ]
test <- prc.pc1[-prc.pc1_index, ]

fit.pc1 = train(income_above_50k ~ .,
             data = train, 
             method = "glm",
             family = "binomial",
             metric = "ROC",
             trControl = ctrl)
```
fit.pc1's accuracy in training the model was .796, and kappa = .4. That is impressive for just one component. 

Now to include all 5 PCs and retrain model: 

```{r}
prc_index <- createDataPartition(prc$income_above_50k, p = 0.80, list = FALSE)
train <- prc[prc.pc1_index, ]
test <- prc[-prc.pc1_index, ]

fit.pca5 = train(income_above_50k ~ .,
             data = train, 
             method = "glm",
             family = "binomial",
             metric = "ROC",
             trControl = ctrl)


```
The addition of other 4 components did not significantly increase performance of logistic regression. This confirms that we bring along only PC1 as we build our predictive model. 





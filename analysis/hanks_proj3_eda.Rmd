---
title: "Project 3"
author: "Charles Hanks & Karol Orozco"
date: "03/16/2023"
output:
  pdf_document:
    df_print: kable
    fig_width: 11
    fig_height: 8
  html_document:
    df_print: paged
---





## Setup
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
# https://www.openml.org/d/1590

options(scipen=999)
raw_income = read_csv("openml_1590.csv", na=c("?"))

income = read_csv("openml_1590.csv", na=c("?")) %>%
  drop_na() %>%
  mutate(income_above_50k = class==">50K") %>%
  select(-class) %>%
  dummy_cols(remove_selected_columns = T)

View(income)
```

## Some questions about the data

Please try at least a few of the following:

* Run PCA on the dataset. How many principal components do you need to explain half the variation? 
* Can you give some interpretation of the principal components?
* Look at the scree plot. How many PCs would you choose based on that?
* Are the first few Principal Components good predictors of income_above_50k?
* How well can you predict income_above_50k using the first 5 or 6 principal components? How about only the first 2?
* Can you gain any insights into the data based on k-means clustering? 
* Can you visualize and interpret some or all of your clusters?
* Using any and all techniques we have learned, build the best predictive model for income_above_50k that you can. What are your best features? What model did you use? What interpretations can you draw?
* What metric can you use to assess model performance? Why is that a good choice of metric in this case?
* What are some key insights you found through your analysis?

Please remember: a statement like "PC2 is not a meaningful predictor for our modeling problem" is a great insight; sometimes things don't work!

10 pts PCA

* analysis and interpretation of factor loadings
* discussion of scree plot and/or analysis of some density plots of PCs
* meaningful interpretation / discussion of conclusions 

10 pts k-means

* discussion for choosing number of clusters
* analysis of cluster centers
* bivariate chart(s) against meaningful variables and/or analysis of density plots
* meaningful interpretation / discussion of conclusions 

10 pts supervised learning

* feature engineering / selection, whether with PCA or otherwise
* interpretation of variable importance, coefficients if applicable
* justification of choice of metric
* discussion of choice or tuning of hyperparameters, if any
* meaningful discussion of predictive power and conclusions from model

Please be prepared to 

* Submit your Rmd + compiled html or pdf, *and*
* Present your findings to the class in a compelling way, speaking for 10 minutes or so. You don't need to cover everything in your analysis that you submit to me, focus on the fun / interesting / compelling highlights or challenges.


####

# BACKGROUND

- Prediction task is to determine whether a person makes over 50k a year. Data from 1994 Census database. 
- fnlwgt: demographic background of the people - people with similar demographic characteristics should have similar weights. 


```{r}
library(skimr)
skim(income)

ggplot(income, aes(x = `capital-loss`)) + geom_histogram()
#I should log capital-gain and capital-loss
#definition: capital gain refers to the increase in the value of a capital asset when it is sold. A capital gain occurs when you sell an asset for more than what you originally paid for it. 

income = income %>% mutate(l_capital_gain = log(`capital-gain`), 
                  l_capital_loss = log(`capital-loss`)) %>% 
                        select(-`capital-gain`,-`capital-loss`)

#moving these cols back to front of dummy cols: 
income = income %>% relocate(l_capital_gain, l_capital_loss)

#I can't deal with these hyphens: 
income = income %>% rename_all(funs(str_replace_all(.,"-","_"))) %>% 
                rename_all(funs(tolower(.)))

```

# Exploratory Data Analysis 

```{r}
#what is education_num? 
#is NA retired? Unemployed? 
raw_income %>% ggplot(aes(x = age, fill = occupation)) + geom_histogram() + facet_wrap(~occupation)
#military is a younger person's occupation 
#exec-manegerial is very normally distributed 
```

Distribution of careers by gender 
```{r}
raw_income %>% 
  group_by(occupation,sex) %>% 
          count() %>% 
          ggplot(aes(x = occupation, y = n, fill = sex)) + geom_col() + coord_flip()
```

```{r}
raw_income %>% group_by(education, workclass) %>% count() %>% ggplot(aes(x = n, y = education, fill = workclass)) + geom_col()
```
How about age vs. relationship? 
```{r}
raw_income %>% group_by(relationship) %>% summarize(avg_age= mean(age))
raw_income %>% group_by(sex) %>% count()
#there are twice as many men in this dataset....may need to downsample by gender later in the game. This may lead to implicit bias toward men making more than 50k in the model. 

raw_income %>% group_by(sex, relationship) %>% count()
#there is 1 female husband, look like this dataset does not take into account the difference between gender and sex


```

